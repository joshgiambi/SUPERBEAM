# SegVol Contour Size Fix

## Problem Identified

SegVol was producing **contours that were too large** - sometimes 3-4x bigger than the input contour.

### Root Cause

The model outputs **probability masks** with values 0-1. The issue was:

1. **Threshold too low**: Using 0.5 threshold captured too many pixels
2. **Model output**: Max probability was only ~0.565, so threshold of 0.5 captured almost everything
3. **Result**: Predicted masks were 3.3x larger than reference (1414 vs 432 points)

### Evidence from Logs

```
ðŸ“Š Reference mask: contour points: 432
ðŸ“Š Predicted mask: max value: 0.565
ðŸ“Š Predicted contour points: 1414  â† 3.3x larger!
ðŸ“Š Sample coordinates: [[366.5, 511.0], [366.5, 510.0]]  â† hitting image edges!
```

## Fixes Applied

### 1. Increased Mask Threshold (Primary Fix)

**File**: [segvol_service.py:266](server/segvol/segvol_service.py#L266)

```python
# OLD
def _mask_to_contour(self, mask: np.ndarray, threshold: float = 0.5):

# NEW
def _mask_to_contour(self, mask: np.ndarray, threshold: float = 0.55):
```

**Effect**: Reduces over-segmentation by requiring higher confidence (0.55 instead of 0.5)

### 2. Fixed Minimum Volume Depth

**File**: [segvol_service.py:202](server/segvol/segvol_service.py#L202)

```python
# OLD
num_interp_slices = max(1, int(distance / spacing[2]))

# NEW - prevents "kernel size greater than input" error
num_interp_slices = max(2, int(distance / spacing[2]))  # Min 4 total depth
```

**Effect**: Ensures volume has at least 4 slices (required by patch size 4Ã—16Ã—16)

### 3. Enhanced Logging

Added detailed logging to track:
- Mask pixel counts
- Contour point counts
- Confidence calculation details
- Threshold effects

## Expected Results

After this fix:

âœ… **Predicted contours should match input size** more closely
âœ… **No more kernel size errors** for adjacent slices
âœ… **Better confidence scores** due to improved area matching
âœ… **Detailed logs** for further debugging if needed

## Testing

### Before Fix
```
Input: 432 points
Output: 1414 points (327% size!)
Confidence: LOW (rejected due to area mismatch)
```

### After Fix
```
Input: 432 points
Output: ~400-500 points (expected range)
Confidence: HIGHER (better area matching)
Result: Visible prediction!
```

## How to Verify

1. **Reload your viewer** (refresh browser)
2. **Draw a contour** on one slice
3. **Navigate to next slice** with SegVol selected
4. **Check logs**:
   ```bash
   tail -f /tmp/segvol-fixed.log | grep "ðŸ“Š\|Confidence"
   ```

### Look for:
- `mask_pixels` should be similar to reference mask size
- `contour_points` should be comparable to input (not 3x larger!)
- `area_ratio` should be > 0.6 (indicating good size match)
- `final confidence` should be > 0.2 (threshold for display)

## Additional Notes

### Why 0.55 threshold?

The model outputs relatively **low probabilities** (max ~0.56-0.60). A threshold of 0.5 includes almost everything. By increasing to 0.55, we:
- Keep only the most confident pixels
- Reduce false positives at boundaries
- Better match the actual anatomy

### Can the threshold be adjusted?

Yes! If contours are now **too small**, decrease the threshold:
- Try 0.53 or 0.52
- Monitor logs to see effect on mask size
- Find sweet spot between over/under-segmentation

### What if it still doesn't work?

Check these in order:
1. **Confidence too low** - May need to adjust confidence calculation weights
2. **Model accuracy** - SegVol may not work well for all anatomy types
3. **Coordinate transforms** - Verify worldâ†”pixel conversions are correct
4. **Fallback to Mem3D** - May give better results for certain cases

## Files Modified

- [server/segvol/segvol_service.py](server/segvol/segvol_service.py)
  - Line 266: Increased threshold 0.5 â†’ 0.55
  - Line 202: Min volume depth 1 â†’ 2
  - Line 287: Added mask-to-contour logging
  - Line 387: Added confidence calculation logging

## Status

âœ… **Service restarted with fixes**
âœ… **Health check passing**
âœ… **Ready for testing**

Try making a prediction now - the contours should be the right size!
